# COUNTDOWN

This repository contains code to reproduce the experiments from our paper "COUNTDOWN: Contextually Sparse Activation Filtering out Unnecessary Weights in Down Projection".

## News
- [08/2025] ðŸš€ COUNTDOWN is accepted to EMNLP 2025 Main Track!

## System Requirements

- Python 3.10+
- CUDA 12.1+ and compatible drivers
- 48GB+ GPU VRAM (24GB+ recommended for larger models)
- Large storage (700GB+ ) for saving trace files of Gated MLP operations

## Setup

```bash
pip install -r requirements.txt
pip install -e .
mkdir -p MODELS DATA results
```

## Folder Structure

```html
COUNTDOWN/
â”œâ”€â”€ countdown/              # Core implementation of COUNTDOWN methods
â”œâ”€â”€ prep_data/              # Data preparation utilities
â”œâ”€â”€ evaluation/             # Evaluation scripts
â”œâ”€â”€ scripts/                # Shell scripts for running experiments
â”œâ”€â”€ config/                 # Configuration files
â”‚   â”œâ”€â”€ d-countdown/        # Configs for D-COUNTDOWN
â”‚   â”œâ”€â”€ m-countdown/        # Configs for M-COUNTDOWN
â”‚   â”œâ”€â”€ cats/               # Configs for CATS baseline
â”‚   â””â”€â”€ ...
â”œâ”€â”€ lm-evaluation-harness/  # Downstream task performance evaluation
â”œâ”€â”€ alpaca_eval/            # Chat performance evaluation framework
â”œâ”€â”€ train.py                # Predictor training script
â”œâ”€â”€ requirements.txt        
â””â”€â”€ setup.py
```

## Reproducing Paper Results

### Complete Workflow

1. Evaluate SPá´µáµˆáµ‰áµƒË¡ downstream task performance with `scripts/prob.sh`
2. Train predictor for D-COUNTDOWN with `train.py`
3. Evaluate SPá´¾Ê³áµƒá¶œ downstream task performance with `scripts/bench_prac.sh`
4. Evaluate chat performance with `scripts/alpaca_gen.sh` and `scripts/alpaca_api.sh` 
5. Run kernel speed benchmarks with `evaluation/benchmark_kernel_test.py`
6. Run E2E speed acceleration measurements with `evaluation/benchmark_generate.py`
7. Generate analysis materials with `evaluation/discussion.py` and `evaluation/benchmark_ternary.py`

### Key Experiment Commands

#### Downstream Task Evaluation (SPá´µáµˆáµ‰áµƒË¡)

```bash
# Required parameters:
# MODEL_NAME: Model identifier from HuggingFace
# Optional parameters:
# CUDA_VISIBLE_DEVICES: GPU device ID (default: 0)
   
MODEL_NAME="meta-llama/Meta-Llama-3.1-8B-Instruct" CUDA_VISIBLE_DEVICES=0 bash ./scripts/prob.sh
```

#### Predictor Training

```bash
# Required parameters:
# MODEL_NAME: Model identifier from HuggingFace
# Optional parameters:
# CUDA_VISIBLE_DEVICES: GPU device ID (default: 0)
   
MODEL_NAME="meta-llama/Meta-Llama-3.1-8B-Instruct" CUDA_VISIBLE_DEVICES=0 bash ./scripts/train.sh
```

#### Practical Performance Evaluation (SPá´¾Ê³áµƒá¶œ)

```bash
# Required parameters:
# MODEL_NAME: Model identifier from HuggingFace
# Optional parameters:
# CUDA_VISIBLE_DEVICES: GPU device ID (default: 0)
   
MODEL_NAME="meta-llama/Meta-Llama-3.1-8B-Instruct" CUDA_VISIBLE_DEVICES=0 bash ./scripts/bench_prac.sh
```

#### Chat Performance Evaluation

```bash
# Step 1: Generate responses
MODEL_NAME="meta-llama/Meta-Llama-3.1-8B-Instruct" CUDA_VISIBLE_DEVICES=0 bash ./scripts/alpaca_gen.sh

# Step 2: Evaluate responses (requires OpenAI API key)
MODEL_NAME="meta-llama/Meta-Llama-3.1-8B-Instruct" OPENAI_API_KEY="your_api_key" bash ./scripts/alpaca_api.sh
```

#### Performance Benchmarking

```bash
# Kernel speed benchmarks
python evaluation/benchmark_kernel_test.py

# End-to-end generation throughput
python evaluation/benchmark_generate.py \
  --model_name meta-llama/Meta-Llama-3.1-8B-Instruct \
  --sparsity_config config/d-countdown/prac/<<TARGET_CONFIG>>.yaml \
  --max_length 512
```

#### Analysis Data Generation

```bash
# Generate data for visualization and comparison analysis
# This script uses hardcoded model parameters and generates data for the CIF/CAF analysis
python evaluation/discussion.py

# Analyze and benchmark ternary weight patterns as alternative predictors
# Compares performance metrics between ternary weights and low-rank approximation
python evaluation/benchmark_ternary.py
```

#### Citation

```
@misc{cheon2025countdowncontextuallysparseactivation,
      title={COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection}, 
      author={Jaewon Cheon and Pilsung Kang},
      year={2025},
      eprint={2505.17701},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.17701}, 
}
```
