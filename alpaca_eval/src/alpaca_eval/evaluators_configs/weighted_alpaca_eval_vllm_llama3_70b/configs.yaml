weighted_alpaca_eval_vllm_llama3_70b:
  prompt_template: "alpaca_eval_clf_gpt4_turbo/alpaca_eval_clf.txt"
  fn_completions: "openai_completions"
  completions_kwargs:
    model_name: "./models/Meta-Llama-3-70B-Instruct" # TODO: replace with path to the model
    max_tokens: 1
    temperature: 1 # temperature should be applied for sampling, so that should make no effect.
    logprobs: true
    top_logprobs: 5
    requires_chatml: true
  fn_completion_parser: "logprob_parser"
  completion_parser_kwargs:
    numerator_token: "m"
    denominator_tokens: ["m", "M"]
    is_binarize: false
  completion_key: "completions_all"
  batch_size: 1
